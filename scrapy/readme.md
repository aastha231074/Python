# What is Scrapy? 
An open source and collaborative framework for extracting the data you need from websites. In a fast, simple, yet extensible way. 

# Why Use Scrapy Over Other Python Web Crawling Frameworks ğŸ•·ï¸

Scrapy is a **full-fledged, production-ready web crawling framework** designed specifically for large-scale data extraction. Compared to ad-hoc tools like `requests + BeautifulSoup` or browser automation tools like Selenium/Playwright, Scrapy shines in **performance, scalability, and maintainability**.

Below are the key benefits of using **Scrapy** over other Python web crawling approaches.

---

## 1. Built for High-Performance Crawling ğŸš€

- **Asynchronous & non-blocking** (built on Twisted)
- Handles **thousands of concurrent requests** efficiently
- No browser overhead â†’ extremely fast for HTML-based websites

**Result:**  
Much faster crawls with lower CPU & memory usage.

---

## 2. Framework, Not Just a Library ğŸ§±

Scrapy gives you an **end-to-end crawling ecosystem**:

- Spider lifecycle management
- Request scheduling & prioritization
- Retry & timeout handling
- Auto throttling & concurrency control
- Middleware & pipeline architecture

ğŸ‘‰ With other tools, you have to manually wire all this together.

---

## 3. Automatic Request Scheduling & Deduplication ğŸ”

- Built-in **request queue**
- Automatic **URL deduplication**
- Depth-based crawling out of the box

No need to track visited URLs manually like in custom crawlers.

---

## 4. Clean Separation of Concerns ğŸ§©

Scrapy enforces a **clean architecture**:

- **Spiders** â†’ what to crawl
- **Middlewares** â†’ how requests/responses are handled
- **Pipelines** â†’ how data is cleaned, validated, stored

This makes:
- Code easier to maintain
- Collaboration easier
- Debugging much simpler

---

## 5. Powerful Data Pipelines ğŸ“¦

- Built-in support for:
  - Validation
  - Cleaning
  - Deduplication
  - Storing to DB / S3 / Files
- Easily plug into:
  - PostgreSQL / MySQL
  - MongoDB
  - Kafka / Queues

Perfect for **production data ingestion systems**.

---

## 6. Smart Throttling & Politeness Controls ğŸ¤

Scrapy helps you avoid bans:

- AutoThrottle
- Download delays
- Per-domain concurrency
- Respect for `robots.txt`

Much harder to manage correctly in custom scripts.

---

## 7. Middleware System = Superpowers ğŸ› ï¸

You can intercept and modify:
- Requests
- Responses
- Headers
- Cookies
- Proxies
- User agents

This is ideal for:
- Proxy rotation
- Auth handling
- Header spoofing
- Anti-bot strategies

---

## 8. Robust Error Handling & Retries âš ï¸

Built-in handling for:
- Timeouts
- Network errors
- HTTP error codes
- Retry logic
- Backoff strategies

You get **stable crawlers** without writing defensive code everywhere.

---

## 9. CLI, Logging & Monitoring Ready ğŸ“Š

- Built-in CLI (`scrapy crawl`, `scrapy shell`)
- Structured logging
- Stats collection (requests/sec, errors, retries)

Easy to integrate with:
- Cron
- Docker
- Airflow
- Kubernetes

---

## 10. Scales Well for Production ğŸ—ï¸

Scrapy is ideal when:
- You have **multiple spiders**
- Crawls run **daily/hourly**
- Data needs to be **clean & consistent**
- Failures must be **recoverable**

This is why Scrapy is widely used in **data engineering & crawling teams**.

---

## Scrapy vs Other Approaches âš”ï¸

| Tool | Best For | Limitation |
|----|----|----|
| requests + BeautifulSoup | Small scripts | No scale, no retries |
| Selenium / Playwright | JS-heavy sites | Slow, resource-heavy |
| Scrapy | Large-scale crawling | JS rendering not native |
| Scrapy + Playwright | JS sites at scale | Slight complexity |

---

## When Scrapy is the Best Choice âœ…

Use Scrapy if:
- Website is **HTML or API based**
- You need **speed & reliability**
- Crawling is **recurring**
- Code needs to be **maintainable**
- Data feeds into **pipelines or dashboards**

# ğŸ•·ï¸ Scrapy Project Guide

A practical guide to getting started with **Scrapy**, testing selectors, building real spiders, and understanding when to choose Scrapy vs Selenium.

---

## ğŸš€ Creating a New Scrapy Project

```bash
scrapy startproject <project_name>

<project_name>/
â”œâ”€â”€ scrapy.cfg
â””â”€â”€ <project_name>/
    â”œâ”€â”€ __init__.py
    â”œâ”€â”€ items.py
    â”œâ”€â”€ middlewares.py
    â”œâ”€â”€ pipelines.py
    â”œâ”€â”€ settings.py
    â””â”€â”€ spiders/
        â””â”€â”€ __init__.py
```

## ğŸ•¸ï¸ Creating a Spider
Navigate to the spiders directory and run:
```bash
scrapy genspider <spider_name> <website_url>
```
Example:
```bash
scrapy genspider quotes quotes.toscrape.com
```
This generates a spider inside:
```bash
<project_name>/spiders/quotes.py
```

## â–¶ï¸ Running a Spider
From the project root:

```bash
scrapy crawl <spider_name>
```
Save output to a file:

```bash
scrapy crawl <spider_name> -O output.json
```
Supported formats: json, csv, jl, xml

## ğŸ§ª Scrapy Shell (Selector Testing & Debugging)
### 1ï¸âƒ£ Install IPython
```bash
pip install ipython
```
### 2ï¸âƒ£ Enable IPython in scrapy.cfg
```
[settings]
shell = ipython
```

### 3ï¸âƒ£ Start the Shell

```bash
scrapy shell
```
#### ğŸ” Using Scrapy Shell
```python
# Fetch a URL
fetch("https://example.com")
# Response Object
response
# ğŸ¯ CSS Selector Examples
response.css("div.quote")
# Get first match:
response.css("div.quote").get()
# Get all matches:
response.css("div.quote").getall()
# Extract text:
response.css("span.text::text").get()
# Extract attribute:
response.css("a::attr(href)").get()

# ğŸ§­ XPath Selector Examples

# Basic XPath:
response.xpath("//div[@class='quote']")
# Get text:
response.xpath("//span[@class='text']/text()").get()
# Get all texts:
response.xpath("//span[@class='text']/text()").getall()
# Get attribute:
response.xpath("//a/@href").get()
# Using contains:
response.xpath("//div[contains(@class,'quote')]")
# Relative XPath:
response.xpath(".//span/text()").get()
```

### ğŸ§‘â€ğŸ’» Real Scrapy Spider Example

Example spider that crawls quotes.toscrape.com
```python
import scrapy

class QuotesSpider(scrapy.Spider):
    name = "quotes"
    start_urls = [
        "https://quotes.toscrape.com/"
    ]

    def parse(self, response):
        for quote in response.css("div.quote"):
            yield {
                "text": quote.css("span.text::text").get(),
                "author": quote.css("small.author::text").get(),
                "tags": quote.css("div.tags a.tag::text").getall(),
            }

        # pagination
        next_page = response.css("li.next a::attr(href)").get()
        if next_page:
            yield response.follow(next_page, callback=self.parse)
```
Run:
```bash
scrapy crawl quotes -O quotes.json
```

```bash
scrapy crawl quotes -O quotes.csv
```
